{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ablation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RESULTS_DIR = Path(\"../experiments/results\")\n",
        "LOGS_DIR = Path(\"../experiments/logs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. System-level comparison (baselines vs RAG + LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_path = RESULTS_DIR / \"metrics.json\"\n",
        "with metrics_path.open() as f:\n",
        "    all_results = json.load(f)\n",
        "\n",
        "all_results\n",
        "\n",
        "df_systems = pd.DataFrame.from_dict(all_results, orient=\"index\")\n",
        "df_systems\n",
        "\n",
        "ax = df_systems[[\"rougeL\", \"job_relevance_mean\", \"resume_alignment_mean\"]].plot(\n",
        "    kind=\"bar\", rot=45\n",
        ")\n",
        "plt.title(\"System Comparison: Baselines vs RAG+LoRA\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Hyperparameter Tuning (LoRA rank, learning rate, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hparam_path = RESULTS_DIR / \"hparam_results.json\"\n",
        "with hparam_path.open() as f:\n",
        "    hparam_results = json.load(f)\n",
        "\n",
        "df_hparams = pd.DataFrame.from_dict(hparam_results, orient=\"index\")\n",
        "df_hparams\n",
        "\n",
        "# Example: effect of lora_r on val_loss\n",
        "if \"lora_r\" in df_hparams.columns:\n",
        "    df_hparams.groupby(\"lora_r\")[\"val_loss\"].mean().plot(kind=\"bar\")\n",
        "    plt.title(\"Effect of LoRA Rank on Validation Loss\")\n",
        "    plt.ylabel(\"Mean validation loss\")\n",
        "    plt.show()\n",
        "\n",
        "# Example: effect of learning_rate on rougeL\n",
        "if \"learning_rate\" in df_hparams.columns:\n",
        "    df_hparams.groupby(\"learning_rate\")[\"rougeL\"].mean().plot(kind=\"bar\")\n",
        "    plt.title(\"Effect of Learning Rate on ROUGE-L\")\n",
        "    plt.ylabel(\"Mean ROUGE-L\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Ablation Study: Impact of RAG and LoRA\n",
        "\n",
        "We compare four variants:\n",
        "1. Prompt-only model (no RAG, no LoRA)\n",
        "2. LoRA fine-tuned model without RAG\n",
        "3. RAG with base model (no LoRA)\n",
        "4. RAG with LoRA (our full system)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ablation_path = RESULTS_DIR / \"ablation_results.json\"\n",
        "with ablation_path.open() as f:\n",
        "    ablation = json.load(f)\n",
        "\n",
        "df_ablation = pd.DataFrame.from_dict(ablation, orient=\"index\")\n",
        "df_ablation\n",
        "\n",
        "df_ablation[[\"rougeL\", \"job_relevance_mean\", \"resume_alignment_mean\"]].plot(\n",
        "    kind=\"bar\", rot=45\n",
        ")\n",
        "plt.title(\"Ablation Study: RAG & LoRA\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Error Analysis and Qualitative Examples\n",
        "\n",
        "We examine examples where the RAG + LoRA system still underperforms or makes mistakes, such as:\n",
        "- Mentioning irrelevant skills.\n",
        "- Overly generic openings.\n",
        "- Weak closing paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qual_path = RESULTS_DIR / \"qualitative_examples.json\"\n",
        "if qual_path.exists():\n",
        "    with qual_path.open() as f:\n",
        "        qual_examples = json.load(f)\n",
        "\n",
        "    # Show a few examples\n",
        "    for ex in qual_examples[:3]:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"JOB:\\n\", ex[\"job_text\"][:600], \"\\n\")\n",
        "        print(\"RESUME:\\n\", ex[\"resume_text\"][:600], \"\\n\")\n",
        "        print(\"REFERENCE COVER LETTER:\\n\", ex[\"reference\"][:600], \"\\n\")\n",
        "        print(\"GENERATED COVER LETTER:\\n\", ex[\"generated\"][:600], \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
